{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = \"/root/.cache/huggingface/zzl/models/cosyvoice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:26:14,539 - modelscope - INFO - PyTorch version 2.3.1+cu121 Found.\n",
      "2025-02-25 10:26:14,542 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2025-02-25 10:26:14,665 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 2a832d566ee6d1ad18205c50f7781c34 and a total number of 980 components indexed\n",
      "/opt/conda/envs/cosyvoice/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('third_party/Matcha-TTS')\n",
    "from cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2\n",
    "from cosyvoice.utils.file_utils import load_wav\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CosyVoice2 Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cosyvoice/lib/python3.10/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n",
      "2025-02-25 10:26:43,035 INFO input frame rate=25\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[0;93m2025-02-25 10:26:50.426492161 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 8 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-02-25 10:26:50.429225403 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-02-25 10:26:50.429233814 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text.cc: festival_Text_init\n",
      "open voice lang map failed\n"
     ]
    }
   ],
   "source": [
    "# import logging\n",
    "\n",
    "# logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "try:\n",
    "    cosyvoice = CosyVoice2(f\"{MODELS_PATH}/pretrained_models/CosyVoice2-0.5B\", load_jit=False, load_trt=False, fp16=False)\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(\"ERROR: \", traceback.format_exc())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]2025-02-25 10:28:15,043 INFO synthesis text 收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。\n",
      "2025-02-25 10:28:24,411 INFO yield speech len 12.32, rtf 0.7603888000760759\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# zero_shot usage\n",
    "prompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)\n",
    "for i, j in enumerate(cosyvoice.inference_zero_shot(\n",
    "    '收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。', \n",
    "    # \" vector quantization（向量量化）是一种将连续的高维数据映射到一个离散的低维空间的技术。具体来说，它将输入的连续数据（比如语音信号或特征向量）映射到一个预先定义的离散“字典”中，这些字典中的元素通常是有限的，因此这能减少输入数据的复杂度，并对数据进行压缩或离散化。\",\n",
    "    # \"在RAG系统中，这种高效的实现往往是通过“分块”来实现的。你可以把它想象成把一本厚书分成几章——这样一来，阅读和理解就轻松多了。同样地，分块技术把大段复杂的文本拆分成更小、更容易处理的片段，让AI能更快、更准确地理解和处理信息。\",\n",
    "    '希望你以后能够做的比我还好呦。',\n",
    "    prompt_speech_16k, \n",
    "    stream=False,\n",
    "    )):\n",
    "    torchaudio.save('./generate_speech/zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]2025-02-25 10:28:28,479 INFO synthesis text 在他讲述那个荒诞故事的过程中，他突然[laughter]停下来，因为他自己也被逗笑了[laughter]。\n",
      "2025-02-25 10:28:31,938 INFO yield speech len 7.8, rtf 0.4434241087008745\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# fine grained control\n",
    "for i, j in enumerate(cosyvoice.inference_cross_lingual(\n",
    "    '在他讲述那个荒诞故事的过程中，他突然[laughter]停下来，因为他自己也被逗笑了[laughter]。',\n",
    "    prompt_speech_16k,\n",
    "    stream=False,\n",
    ")):\n",
    "    torchaudio.save('./generate_speech/fine_grained_control_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]2025-02-25 10:28:35,710 INFO synthesis text 收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。\n",
      "2025-02-25 10:28:40,339 INFO yield speech len 10.6, rtf 0.43673146445796174\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# instruct usage\n",
    "for i, j in enumerate(cosyvoice.inference_instruct2(\n",
    "    '收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。',\n",
    "    '用粤语说这句话',\n",
    "    prompt_speech_16k,\n",
    "    stream=False,\n",
    ")):\n",
    "    torchaudio.save('./generate_speech/instruct_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:28:41,792 INFO get tts_text generator, will skip text_normalize!\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]2025-02-25 10:28:41,796 INFO get tts_text generator, will return _extract_text_token_generator!\n",
      "2025-02-25 10:28:41,868 INFO synthesis text <generator object text_generator at 0x716dbafc6d50>\n",
      "2025-02-25 10:28:41,872 INFO append 5 text token 15 speech token\n",
      "2025-02-25 10:28:41,873 INFO append 5 text token 15 speech token\n",
      "2025-02-25 10:28:41,874 INFO append 5 text token 15 speech token\n",
      "2025-02-25 10:28:41,875 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,876 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,878 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,879 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,880 INFO append 5 text token 15 speech token\n",
      "2025-02-25 10:28:41,881 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,882 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,883 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,884 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,885 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,887 INFO append 5 text token 15 speech token\n",
      "2025-02-25 10:28:41,888 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,889 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,890 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,891 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,892 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,893 INFO append 5 text token 12 speech token\n",
      "2025-02-25 10:28:41,972 INFO fill_token index 3 next fill_token index 19\n",
      "2025-02-25 10:28:41,974 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:41,974 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,976 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:41,976 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,978 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:41,978 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,980 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:41,980 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:41,982 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:41,983 INFO append 5 text token\n",
      "2025-02-25 10:28:42,237 INFO fill_token index 19 next fill_token index 35\n",
      "2025-02-25 10:28:42,239 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,239 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:42,241 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,241 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:42,243 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,243 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:42,245 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,245 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:42,247 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,247 INFO append 5 text token\n",
      "2025-02-25 10:28:42,496 INFO fill_token index 35 next fill_token index 51\n",
      "2025-02-25 10:28:42,497 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,498 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:42,499 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,500 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:42,501 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,502 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:42,503 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,504 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:42,505 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,506 INFO append 5 text token\n",
      "2025-02-25 10:28:42,755 INFO fill_token index 51 next fill_token index 67\n",
      "2025-02-25 10:28:42,756 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,757 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:42,758 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,759 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:42,760 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,761 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:42,762 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,763 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:42,764 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:42,765 INFO append 5 text token\n",
      "2025-02-25 10:28:43,011 INFO fill_token index 67 next fill_token index 83\n",
      "2025-02-25 10:28:43,013 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,013 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:43,015 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,015 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:43,017 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,018 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:43,019 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,020 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:43,021 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,022 INFO append 5 text token\n",
      "2025-02-25 10:28:43,264 INFO fill_token index 83 next fill_token index 99\n",
      "2025-02-25 10:28:43,266 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,266 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:43,268 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,269 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:43,270 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,271 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:43,272 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,273 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:43,274 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,275 INFO append 5 text token\n",
      "2025-02-25 10:28:43,520 INFO fill_token index 99 next fill_token index 115\n",
      "2025-02-25 10:28:43,522 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,522 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:43,524 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,524 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:43,526 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,526 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:43,528 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,528 INFO not enough text token to decode, wait for more\n",
      "2025-02-25 10:28:43,530 INFO get fill token, need to append more text token\n",
      "2025-02-25 10:28:43,530 INFO append 5 text token\n",
      "2025-02-25 10:28:43,777 INFO fill_token index 115 next fill_token index 131\n",
      "2025-02-25 10:28:43,777 INFO no more text token, decode until met eos\n",
      "2025-02-25 10:28:47,034 INFO yield speech len 11.0, rtf 0.469620097767223\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# bistream usage\n",
    "# 使用双流时，您可以使用生成器作为输入，这在使用文本 llm 模型作为输入时非常有用\n",
    "# 注意：由于 llm 无法处理任意长度的句子，因此您仍需掌握一些基本的句子分割逻辑\n",
    "def text_generator():\n",
    "    yield '收到好友从远方寄来的生日礼物，'\n",
    "    yield '那份意外的惊喜与深深的祝福'\n",
    "    yield '让我心中充满了甜蜜的快乐，'\n",
    "    yield '笑容如花儿般绽放。'\n",
    "\n",
    "for i, j in enumerate(cosyvoice.inference_zero_shot(\n",
    "    text_generator(),\n",
    "    '希望你以后能够做的比我还好呦。',\n",
    "    prompt_speech_16k,\n",
    "    stream=False,\n",
    ")):    \n",
    "    torchaudio.save('./generate_speech/bistream_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CosyVoice Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:28:52,481 INFO input frame rate=50\n",
      "\u001b[0;93m2025-02-25 10:28:58.060171249 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 12 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-02-25 10:28:58.061505436 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-02-25 10:28:58.061514878 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open voice lang map failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "festival_initialize() called more than once\n"
     ]
    }
   ],
   "source": [
    "cosyvoice1 = CosyVoice(\n",
    "    f\"{MODELS_PATH}/pretrained_models/CosyVoice-300M-SFT\",\n",
    "    load_jit=False,\n",
    "    load_trt=False, \n",
    "    fp16=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中文女', '中文男', '日语男', '粤语女', '英文女', '英文男', '韩语女']\n"
     ]
    }
   ],
   "source": [
    "# sft usage\n",
    "print(cosyvoice1.list_available_spks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]2025-02-25 10:29:24,294 INFO synthesis text 你好，我是通义生成式语音大模型，请问有什么可以帮您的吗？\n",
      "2025-02-25 10:29:27,380 INFO yield speech len 5.0967800453514736, rtf 0.6054171600754549\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# change stream=True for chunk stream inference\n",
    "for i,j in enumerate(cosyvoice1.inference_sft(\n",
    "    '你好，我是通义生成式语音大模型，请问有什么可以帮您的吗？',\n",
    "    '中文女',\n",
    "    stream=False,\n",
    ")):\n",
    "    torchaudio.save('./generate_speech/cosyvoice1/sft_{}.wav'.format(i), j['tts_speech'], cosyvoice1.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:39:13,092 INFO input frame rate=50\n",
      "\u001b[0;93m2025-02-25 10:39:14.097100357 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 12 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-02-25 10:39:14.098439293 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-02-25 10:39:14.098448068 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "festival_initialize() called more than once\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open voice lang map failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stft(torch.FloatTensor[1, 77566], n_fft=1024, hop_length=256, win_length=1024, window=torch.FloatTensor{[1920]}, normalized=0, onesided=1, return_complex=1) : expected a 1D window tensor of size equal to win_length=1024, but got window with size [1920]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# zero-shot usage\u001b[39;00m\n\u001b[1;32m      3\u001b[0m prompt_speech_16k \u001b[38;5;241m=\u001b[39m load_wav(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./asset/zero_shot_prompt.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m16000\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cosyvoice1\u001b[38;5;241m.\u001b[39minference_zero_shot(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m希望你以后能够做的比我还好呦。\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     prompt_speech_16k,\n\u001b[1;32m      8\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m )):\n\u001b[1;32m     10\u001b[0m     torchaudio\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./generate_speech/cosyvoice1/zero_shot_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i), j[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtts_speech\u001b[39m\u001b[38;5;124m'\u001b[39m], cosyvoice1\u001b[38;5;241m.\u001b[39msample_rate)\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/cosyvoice/cli/cosyvoice.py:88\u001b[0m, in \u001b[0;36mCosyVoice.inference_zero_shot\u001b[0;34m(self, tts_text, prompt_text, prompt_speech_16k, stream, speed, text_frontend)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i, Generator)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(i) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_text):\n\u001b[1;32m     87\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynthesis text \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m too short than prompt text \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, this may lead to bad performance\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i, prompt_text))\n\u001b[0;32m---> 88\u001b[0m model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrontend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrontend_zero_shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_speech_16k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     90\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynthesis text \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i))\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/cosyvoice/cli/frontend.py:167\u001b[0m, in \u001b[0;36mCosyVoiceFrontEnd.frontend_zero_shot\u001b[0;34m(self, tts_text, prompt_text, prompt_speech_16k, resample_rate)\u001b[0m\n\u001b[1;32m    165\u001b[0m prompt_text_token, prompt_text_token_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_text_token(prompt_text)\n\u001b[1;32m    166\u001b[0m prompt_speech_resample \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResample(orig_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m, new_freq\u001b[38;5;241m=\u001b[39mresample_rate)(prompt_speech_16k)\n\u001b[0;32m--> 167\u001b[0m speech_feat, speech_feat_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_speech_feat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_speech_resample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m speech_token, speech_token_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_speech_token(prompt_speech_16k)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resample_rate \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m24000\u001b[39m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# cosyvoice2, force speech_feat % speech_token = 2\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/cosyvoice/cli/frontend.py:122\u001b[0m, in \u001b[0;36mCosyVoiceFrontEnd._extract_speech_feat\u001b[0;34m(self, speech)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_extract_speech_feat\u001b[39m(\u001b[38;5;28mself\u001b[39m, speech):\n\u001b[0;32m--> 122\u001b[0m     speech_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeat_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeech\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    123\u001b[0m     speech_feat \u001b[38;5;241m=\u001b[39m speech_feat\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    124\u001b[0m     speech_feat_len \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([speech_feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/third_party/Matcha-TTS/matcha/utils/audio.py:63\u001b[0m, in \u001b[0;36mmel_spectrogram\u001b[0;34m(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center)\u001b[0m\n\u001b[1;32m     57\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m     58\u001b[0m     y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;28mint\u001b[39m((n_fft \u001b[38;5;241m-\u001b[39m hop_size) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mint\u001b[39m((n_fft \u001b[38;5;241m-\u001b[39m hop_size) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     60\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m spec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhann_window\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43monesided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m spec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(spec\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1e-9\u001b[39m))\n\u001b[1;32m     79\u001b[0m spec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(mel_basis[\u001b[38;5;28mstr\u001b[39m(fmax) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(y\u001b[38;5;241m.\u001b[39mdevice)], spec)\n",
      "File \u001b[0;32m/opt/conda/envs/cosyvoice/lib/python3.10/site-packages/torch/functional.py:665\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[0;32m--> 665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stft(torch.FloatTensor[1, 77566], n_fft=1024, hop_length=256, win_length=1024, window=torch.FloatTensor{[1920]}, normalized=0, onesided=1, return_complex=1) : expected a 1D window tensor of size equal to win_length=1024, but got window with size [1920]"
     ]
    }
   ],
   "source": [
    "cosyvoice1 = CosyVoice(f'{MODELS_PATH}/pretrained_models/CosyVoice-300M')\n",
    "# zero-shot usage\n",
    "prompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)\n",
    "for i, j in enumerate(cosyvoice1.inference_zero_shot(\n",
    "    '收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。',\n",
    "    '希望你以后能够做的比我还好呦。',\n",
    "    prompt_speech_16k,\n",
    "    stream=False,\n",
    ")):\n",
    "    torchaudio.save('./generate_speech/cosyvoice1/zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice1.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min value is  tensor(-1.0054)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stft(torch.FloatTensor[1, 303949], n_fft=1024, hop_length=256, win_length=1024, window=torch.FloatTensor{[1920]}, normalized=0, onesided=1, return_complex=1) : expected a 1D window tensor of size equal to win_length=1024, but got window with size [1920]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# cross-lingual usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m prompt_speech_16k \u001b[38;5;241m=\u001b[39m load_wav(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./asset/cross_lingual_prompt.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m16_000\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cosyvoice1\u001b[38;5;241m.\u001b[39minference_cross_lingual(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<|en|>And then later on, fully acquiring that company. So keeping management in line, interest in line with the asset that\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms coming into the family is a reason why sometimes we don\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt buy the whole thing.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     prompt_speech_16k,\n\u001b[1;32m      6\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )):\n\u001b[1;32m      8\u001b[0m     torchaudio\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./generate_speech/cosyvoice1/cross_linugal_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i), j[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtts_speech\u001b[39m\u001b[38;5;124m'\u001b[39m], sample_rate\u001b[38;5;241m=\u001b[39mcosyvoice1\u001b[38;5;241m.\u001b[39msample_rate)\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/cosyvoice/cli/cosyvoice.py:97\u001b[0m, in \u001b[0;36minference_cross_lingual\u001b[0;34m(self, tts_text, prompt_speech_16k, stream, speed, text_frontend)\u001b[0m\n\u001b[1;32m     95\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myield speech len \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, rtf \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(speech_len, (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m speech_len))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m model_output\n\u001b[0;32m---> 97\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/cosyvoice/cli/frontend.py:184\u001b[0m, in \u001b[0;36mCosyVoiceFrontEnd.frontend_cross_lingual\u001b[0;34m(self, tts_text, prompt_speech_16k, resample_rate)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrontend_cross_lingual\u001b[39m(\u001b[38;5;28mself\u001b[39m, tts_text, prompt_speech_16k, resample_rate):\n\u001b[0;32m--> 184\u001b[0m     model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrontend_zero_shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtts_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_speech_16k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# in cross lingual mode, we remove prompt in llm\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/cosyvoice/cli/frontend.py:167\u001b[0m, in \u001b[0;36mCosyVoiceFrontEnd.frontend_zero_shot\u001b[0;34m(self, tts_text, prompt_text, prompt_speech_16k, resample_rate)\u001b[0m\n\u001b[1;32m    165\u001b[0m prompt_text_token, prompt_text_token_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_text_token(prompt_text)\n\u001b[1;32m    166\u001b[0m prompt_speech_resample \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResample(orig_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m, new_freq\u001b[38;5;241m=\u001b[39mresample_rate)(prompt_speech_16k)\n\u001b[0;32m--> 167\u001b[0m speech_feat, speech_feat_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_speech_feat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_speech_resample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m speech_token, speech_token_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_speech_token(prompt_speech_16k)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resample_rate \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m24000\u001b[39m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# cosyvoice2, force speech_feat % speech_token = 2\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/cosyvoice/cli/frontend.py:122\u001b[0m, in \u001b[0;36mCosyVoiceFrontEnd._extract_speech_feat\u001b[0;34m(self, speech)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_extract_speech_feat\u001b[39m(\u001b[38;5;28mself\u001b[39m, speech):\n\u001b[0;32m--> 122\u001b[0m     speech_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeat_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeech\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    123\u001b[0m     speech_feat \u001b[38;5;241m=\u001b[39m speech_feat\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    124\u001b[0m     speech_feat_len \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([speech_feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/third_party/Matcha-TTS/matcha/utils/audio.py:63\u001b[0m, in \u001b[0;36mmel_spectrogram\u001b[0;34m(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center)\u001b[0m\n\u001b[1;32m     57\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m     58\u001b[0m     y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;28mint\u001b[39m((n_fft \u001b[38;5;241m-\u001b[39m hop_size) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mint\u001b[39m((n_fft \u001b[38;5;241m-\u001b[39m hop_size) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     60\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m spec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhann_window\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43monesided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m spec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(spec\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1e-9\u001b[39m))\n\u001b[1;32m     79\u001b[0m spec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(mel_basis[\u001b[38;5;28mstr\u001b[39m(fmax) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(y\u001b[38;5;241m.\u001b[39mdevice)], spec)\n",
      "File \u001b[0;32m/opt/conda/envs/cosyvoice/lib/python3.10/site-packages/torch/functional.py:665\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[0;32m--> 665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stft(torch.FloatTensor[1, 303949], n_fft=1024, hop_length=256, win_length=1024, window=torch.FloatTensor{[1920]}, normalized=0, onesided=1, return_complex=1) : expected a 1D window tensor of size equal to win_length=1024, but got window with size [1920]"
     ]
    }
   ],
   "source": [
    "# cross-lingual usage\n",
    "prompt_speech_16k = load_wav('./asset/cross_lingual_prompt.wav', 16_000)\n",
    "for i, j in enumerate(cosyvoice1.inference_cross_lingual(\n",
    "    '<|en|>And then later on, fully acquiring that company. So keeping management in line, interest in line with the asset that\\'s coming into the family is a reason why sometimes we don\\'t buy the whole thing.',\n",
    "    prompt_speech_16k,\n",
    "    stream=False,\n",
    ")):\n",
    "    torchaudio.save('./generate_speech/cosyvoice1/cross_linugal_{}.wav'.format(i), j['tts_speech'], sample_rate=cosyvoice1.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stft(torch.FloatTensor[1, 77566], n_fft=1024, hop_length=256, win_length=1024, window=torch.FloatTensor{[1920]}, normalized=0, onesided=1, return_complex=1) : expected a 1D window tensor of size equal to win_length=1024, but got window with size [1920]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt_speech_16k \u001b[38;5;241m=\u001b[39m load_wav(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./asset/zero_shot_prompt.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m16000\u001b[39m)\n\u001b[1;32m      3\u001b[0m source_speech_16k \u001b[38;5;241m=\u001b[39m load_wav(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./asset/cross_lingual_prompt.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m16000\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cosyvoice1\u001b[38;5;241m.\u001b[39minference_vc(\n\u001b[1;32m      5\u001b[0m     source_speech_16k, \n\u001b[1;32m      6\u001b[0m     prompt_speech_16k,\n\u001b[1;32m      7\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     )):\n\u001b[1;32m      9\u001b[0m     torchaudio\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./generate_speech/cosyvoice1/vc_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i), j[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtts_speech\u001b[39m\u001b[38;5;124m'\u001b[39m], sample_rate\u001b[38;5;241m=\u001b[39mcosyvoice1\u001b[38;5;241m.\u001b[39msample_rate)\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/cosyvoice/cli/cosyvoice.py:126\u001b[0m, in \u001b[0;36mCosyVoice.inference_vc\u001b[0;34m(self, source_speech_16k, prompt_speech_16k, stream, speed)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minference_vc\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_speech_16k, prompt_speech_16k, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, speed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m     model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrontend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrontend_vc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_speech_16k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_speech_16k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvc(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_input, stream\u001b[38;5;241m=\u001b[39mstream, speed\u001b[38;5;241m=\u001b[39mspeed):\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/cosyvoice/cli/frontend.py:210\u001b[0m, in \u001b[0;36mCosyVoiceFrontEnd.frontend_vc\u001b[0;34m(self, source_speech_16k, prompt_speech_16k, resample_rate)\u001b[0m\n\u001b[1;32m    208\u001b[0m prompt_speech_token, prompt_speech_token_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_speech_token(prompt_speech_16k)\n\u001b[1;32m    209\u001b[0m prompt_speech_resample \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResample(orig_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m, new_freq\u001b[38;5;241m=\u001b[39mresample_rate)(prompt_speech_16k)\n\u001b[0;32m--> 210\u001b[0m prompt_speech_feat, prompt_speech_feat_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_speech_feat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_speech_resample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_spk_embedding(prompt_speech_16k)\n\u001b[1;32m    212\u001b[0m source_speech_token, source_speech_token_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_speech_token(source_speech_16k)\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/cosyvoice/cli/frontend.py:122\u001b[0m, in \u001b[0;36mCosyVoiceFrontEnd._extract_speech_feat\u001b[0;34m(self, speech)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_extract_speech_feat\u001b[39m(\u001b[38;5;28mself\u001b[39m, speech):\n\u001b[0;32m--> 122\u001b[0m     speech_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeat_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeech\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    123\u001b[0m     speech_feat \u001b[38;5;241m=\u001b[39m speech_feat\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    124\u001b[0m     speech_feat_len \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([speech_feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/workspace/labor/audio/CosyVoice/third_party/Matcha-TTS/matcha/utils/audio.py:63\u001b[0m, in \u001b[0;36mmel_spectrogram\u001b[0;34m(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center)\u001b[0m\n\u001b[1;32m     57\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m     58\u001b[0m     y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;28mint\u001b[39m((n_fft \u001b[38;5;241m-\u001b[39m hop_size) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mint\u001b[39m((n_fft \u001b[38;5;241m-\u001b[39m hop_size) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     60\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m spec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhann_window\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43monesided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m spec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(spec\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1e-9\u001b[39m))\n\u001b[1;32m     79\u001b[0m spec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(mel_basis[\u001b[38;5;28mstr\u001b[39m(fmax) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(y\u001b[38;5;241m.\u001b[39mdevice)], spec)\n",
      "File \u001b[0;32m/opt/conda/envs/cosyvoice/lib/python3.10/site-packages/torch/functional.py:665\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[0;32m--> 665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stft(torch.FloatTensor[1, 77566], n_fft=1024, hop_length=256, win_length=1024, window=torch.FloatTensor{[1920]}, normalized=0, onesided=1, return_complex=1) : expected a 1D window tensor of size equal to win_length=1024, but got window with size [1920]"
     ]
    }
   ],
   "source": [
    "# vc usage\n",
    "prompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)\n",
    "source_speech_16k = load_wav('./asset/cross_lingual_prompt.wav', 16000)\n",
    "for i, j in enumerate(cosyvoice1.inference_vc(\n",
    "    source_speech_16k, \n",
    "    prompt_speech_16k,\n",
    "    stream=False,\n",
    "    )):\n",
    "    torchaudio.save(\"./generate_speech/cosyvoice1/vc_{}\".format(i), j['tts_speech'], sample_rate=cosyvoice1.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:55:31,053 INFO input frame rate=50\n",
      "\u001b[0;93m2025-02-25 10:55:31.795762984 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 12 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-02-25 10:55:31.797126065 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-02-25 10:55:31.797135432 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "festival_initialize() called more than once\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open voice lang map failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]2025-02-25 10:55:33,938 INFO synthesis text 在面对挑战时，他展现了非凡的<strong>勇气</strong>与<strong>智慧</strong>。\n",
      "2025-02-25 10:55:36,906 INFO yield speech len 5.491519274376417, rtf 0.540442898255196\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.07s/it]\n"
     ]
    }
   ],
   "source": [
    "cosyvoice1 = CosyVoice('{}/pretrained_models/CosyVoice-300M-Instruct'.format(MODELS_PATH), load_jit=False, load_trt=False, fp16=False)\n",
    "# instruct usage\n",
    "for i, j in enumerate(cosyvoice1.inference_instruct(\n",
    "    '在面对挑战时，他展现了非凡的<strong>勇气</strong>与<strong>智慧</strong>。',\n",
    "    '中文女',\n",
    "    'Theo \\'Crimson\\', is a fiery, passionate rebel leader. Fights with fervor for justice, but struggles with impulsiveness.',\n",
    "    stream=False,\n",
    ")):\n",
    "    torchaudio.save('./generate_speech/cosyvoice1/instruct_{}.wav'.format(i), j['tts_speech'], cosyvoice1.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosyvoice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
